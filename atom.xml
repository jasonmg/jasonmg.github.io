<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jason&#39;s Blog</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-09-01T15:53:46.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>jason</name>
    <email>ji_mingjiang@163.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>license</title>
    <link href="http://yoursite.com/2017/09/01/license/"/>
    <id>http://yoursite.com/2017/09/01/license/</id>
    <published>2017-09-01T13:42:15.000Z</published>
    <updated>2017-09-01T15:53:46.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GPL-GNU-Public-General-License"><a href="#GPL-GNU-Public-General-License" class="headerlink" title="GPL(GNU Public General License)"></a>GPL(GNU Public General License)</h1><p>强开源约束授权</p>
<p>GPL的出发点是代码的开源/免费使用和引用/修改/衍生代码的开源/免费使用，但不允许修改后和衍生的代码做为闭源的商业软件发布和销售。</p>
<p>GPL协议的主要内容是只要在一个软件中使用(“使用”指类库引用，修改后的代码或者衍生代码)GPL 协议的产品，则该软件产品必须也采用GPL协议，既必须也是开源和免费, 这就是所谓的 <strong>”传染性”</strong>。</p>
<h1 id="LGPL（GNU-Lesser-General-Public-License）"><a href="#LGPL（GNU-Lesser-General-Public-License）" class="headerlink" title="LGPL（GNU Lesser General Public License）"></a>LGPL（GNU Lesser General Public License）</h1><p>LGPL是GPL的一个为主要为<strong>类库使用</strong>设计的开源协议。 </p>
<p>和GPL要求任何使用/修改/衍生之GPL类库的的软件必须采用GPL协议不同，LGPL允许商业软件通过<strong>类库引用(link)方式</strong>使用LGPL类库而<strong>不需要开源</strong>商业软件的代码。这使得采用LGPL协议的开源代码可以被商业软件作为类库引用并发布和销售。</p>
<p>但是如果<strong>修改（不只是引用）</strong>LGPL协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用LGPL协议。</p>
<p>因此LGPL协议的开源代码很适合作为第三方类库被商业软件引用，但不适合希望以LGPL协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。<br>GPL/LGPL都保障原作者的知识产权，避免有人利用开源代码复制并开发类似的产品。</p>
<h1 id="MPL-License（Mozilla-Public-License）"><a href="#MPL-License（Mozilla-Public-License）" class="headerlink" title="MPL License（Mozilla Public License）"></a>MPL License（Mozilla Public License）</h1><p>弱开源约束授权</p>
<p>允许免费重发布、免费修改，但要求修改后的代码版权归软件的发起者。这种授权维护了商业软件的利益，它要求基于这种软件的修改无偿贡献版权给该软件。但MPL是允许修改，无偿使用的。</p>
<p>MPL软件对链接没有要求。(要求假如你修改了一个基于MPL协议的源代码，则必须列入或公开你所做的修改，假如其他源代码不是基于MPL则不需要公开其源代码)</p>
<h1 id="MIT-Licence"><a href="#MIT-Licence" class="headerlink" title="MIT Licence"></a>MIT Licence</h1><p>MIT是和BSD一样宽范的许可协议，作者只想保留版权，而无任何其他了限制。<br>也就是说，你必须在你的发行版里<strong>包含原许可协议</strong>的声明，无论你是以二进制发布的还是以源代码发布的。</p>
<h1 id="BSD-Licence"><a href="#BSD-Licence" class="headerlink" title="BSD Licence"></a>BSD Licence</h1><p>BSD开源协议是一个给于使用者很大自由的协议。<br>基本上使用者可以”为所欲为”，可以自由的使用，修改源代码，也可以将修改后的代码作为开源或者专有软件再发布。</p>
<p>但”为所欲为”的前提当你发布使用了BSD协议的代码，或者以BSD协议代码为基础做二次开发自己的产品时，需要满足三个条件：</p>
<ol>
<li>如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的BSD协议。</li>
<li>如果再发布的只是二进制类库/软件，则需要在类库/软件的文档和版权声明中包含原来代码中的BSD协议。</li>
<li>不可以用开源代码的作者/机构名字和原来产品的名字做市场推广。</li>
</ol>
<p>BSD 代码鼓励代码共享，但需要尊重代码作者的著作权, BSD由于允许使用者修改和重新发布代码，也允许使用或在BSD代码上开发商业软件发布和销售，因此是对商业集成很友好的协议。</p>
<p>而很多的公司企业在选用开源产品的时候都首选BSD协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者二次开发。</p>
<h1 id="Apache-Licence"><a href="#Apache-Licence" class="headerlink" title="Apache Licence"></a>Apache Licence</h1><p>Apache Licence是著名的非盈利开源组织Apache采用的协议。</p>
<p>该协议和BSD类似，同样鼓励代码共享和尊重原作者的著作权，同样允许代码修改，再发布（作为开源或商业软件）。</p>
<p>需要满足的条件也和BSD类似                </p>
<ol>
<li>需要给代码的用户一份Apache Licence</li>
<li>如果你修改了代码，需要再被修改的文件中说明。</li>
<li>在延伸的代码中（修改和有源代码衍生的代码中）需要带有原来代码中的协议，商标，专利声明和其他原来作者规定需要包含的说明。</li>
<li>如果再发布的产品中包含一个Notice文件，则在Notice文件中需要带有Apache Licence。</li>
</ol>
<p>Apache Licence也是对商业应用友好的许可。</p>
<hr>
<p>最后奉上阮一峰的一张<a href="http://www.ruanyifeng.com/blog/2011/05/how_to_choose_free_software_licenses.html" target="_blank" rel="external">图</a>，简单清晰。<br><img src="/2017/09/01/license/license.png" alt="license"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;GPL-GNU-Public-General-License&quot;&gt;&lt;a href=&quot;#GPL-GNU-Public-General-License&quot; class=&quot;headerlink&quot; title=&quot;GPL(GNU Public General License)&quot;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>hadoop</title>
    <link href="http://yoursite.com/2017/09/01/hadoop/"/>
    <id>http://yoursite.com/2017/09/01/hadoop/</id>
    <published>2017-09-01T08:42:54.000Z</published>
    <updated>2017-09-01T15:52:37.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hadoop-Yarn"><a href="#hadoop-Yarn" class="headerlink" title="hadoop Yarn"></a><a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/" target="_blank" rel="external">hadoop Yarn</a></h1>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;hadoop-Yarn&quot;&gt;&lt;a href=&quot;#hadoop-Yarn&quot; class=&quot;headerlink&quot; title=&quot;hadoop Yarn&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://www.ibm.com/developerworks/cn/openso
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>framework</title>
    <link href="http://yoursite.com/2017/08/28/framework/"/>
    <id>http://yoursite.com/2017/08/28/framework/</id>
    <published>2017-08-28T06:16:11.000Z</published>
    <updated>2017-09-01T15:51:04.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Web-App-Architectures"><a href="#Web-App-Architectures" class="headerlink" title="Web App Architectures"></a><a href="https://michaelwashburnjr.com/an-analysis-of-web-app-architecture/" target="_blank" rel="external">Web App Architectures</a></h1>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Web-App-Architectures&quot;&gt;&lt;a href=&quot;#Web-App-Architectures&quot; class=&quot;headerlink&quot; title=&quot;Web App Architectures&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://michae
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>10 Tips for Writing Better Code</title>
    <link href="http://yoursite.com/2017/08/25/10-Tips-for-Writing-Better-Code/"/>
    <id>http://yoursite.com/2017/08/25/10-Tips-for-Writing-Better-Code/</id>
    <published>2017-08-25T11:42:24.000Z</published>
    <updated>2017-08-25T13:02:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>好的代码应该是简洁易读，容易理解，容易debug 和修改，最重要的是bug少，虽然写代码的时候比较费一点时间，但是这是相当值得的，因为你会花更少的时间去维护，并且你的代码可重用性是相当强。</p>
<p>实际上我们可以认为好的代码是 可重用的，代码可能只是你段时间内完成工作任务或者特定功能的目标，但是没有人去重用它（包括你自己），一定是你写的代码有什么问题，比如说太复杂了，太局部于某一方面，及其有可能在某些情况下宕掉，或者别人根本不信任你的代码。</p>
<p>我认为如果持之以恒的坚守如下的代码规范，能让的代码看上去越来越优雅。</p>
<ol>
<li><p>遵守单一职责<br><strong>函数</strong> 是最重要的程序员利器，越是可以重用，你可以写越少的代码，并且越高的稳定性。小而简的函数很易于遵循单一职责。</p>
</li>
<li><p>尽可能少用公共变量<br>你应该尽可能少的在函数之间共享变量，不管是对象内函数调用对象的成员变量还是全局变量，尽可能的通过参数传递变量，这样的话，代码会变得非常容易的理解和重用。</p>
</li>
<li><p>缩小副作用的区域范围<br>理想情况下，副作用（打印到终端，改变外部变量状态，io操作等等）应该放在单独的模块下，不应该分散在项目当中，并且，函数中的副作用通常是违反单一职责原则。</p>
</li>
<li><p>偏向于不可变的对象／变量<br>如果对象的状态只有在初始化的时候设置一次，之后就不可能再被更改，那么这就会让debug变的非常容易，因为如果初始状态没有问题，中间没有可能被别的程序修改，你的思维就不会交织在一起，就像多线程编程容易出错但又难于重现的原因，同时这样也会大大的降低程序的复杂度。</p>
</li>
<li><p>接口优于类<br>接口作为参数，将提高函数的重用性，因为你可以对接口实现类进行更改的同时，保证依赖接口的函数不做更改。</p>
</li>
<li><p>模块化<br>如果可能，应该把一个大的项目分割成小的独立的模块，这样能保证模块的重用。<br>减少模块间的依赖关系</p>
<ul>
<li>每个模块应该有单一的定义良好的职责。</li>
<li>不要重复你自己。</li>
<li>你应该努力尽可能的保证你的项目<strong>小</strong>而<strong>定义良好</strong></li>
</ul>
</li>
<li><p>尽量避免继承</p>
</li>
<li><p>设计和开的同时不要忘了测试<br>我并不是测试驱动开发的推崇者，但是单元测试可以让你的代码更自然的遵循一些参考，同时也及早的帮你发现很多错误，避免写一些无意义的测试，好的编程以为着更高层级的测试，集成测试或者功能测试，这些都非常有利于发现bug。</p>
</li>
<li><p>尽量用标准的类库而不是自己造一个<br>我曾见过非常多的比标准库更好的std::vector 或者 std::string, 但是这样总是会浪费你的时间和精力，除了你及其有可能引入新的bug，其它的开发者也基本上不会用你的代码版本。</p>
</li>
<li><p>避免写新的代码<br>最好的编程是不写代码，你写越多的代码，你就写越多的bug，并且越来越难以发现。 在开始写代码之前问一下你自己，有没有别的什么工具可以实现这个功能？你真的需要自己写一个这样的功能而不是用一个经过良好测试的已经存在的代码？</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好的代码应该是简洁易读，容易理解，容易debug 和修改，最重要的是bug少，虽然写代码的时候比较费一点时间，但是这是相当值得的，因为你会花更少的时间去维护，并且你的代码可重用性是相当强。&lt;/p&gt;
&lt;p&gt;实际上我们可以认为好的代码是 可重用的，代码可能只是你段时间内完成工作
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>spark基础</title>
    <link href="http://yoursite.com/2017/08/24/spark%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2017/08/24/spark基础/</id>
    <published>2017-08-24T06:43:38.000Z</published>
    <updated>2017-09-01T07:30:07.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DStreamDAG"><a href="#DStreamDAG" class="headerlink" title="DStreamDAG"></a><a href="https://github.com/lw-lin/CoolplaySpark/blob/master/Spark%20Streaming%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E7%B3%BB%E5%88%97/1.1%20DStream%2C%20DStreamGraph%20%E8%AF%A6%E8%A7%A3.md" target="_blank" rel="external">DStreamDAG</a></h1><h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><p>Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel.There are two ways to create RDDs: parallelizing an existing collection in your driver program, or referencing a dataset in an external storage system.</p>
<p>One important parameter for parallel collections is the number of partitions to cut the dataset into. Spark will run one task for each partition of the cluster. Typically you want 2-4 partitions for each CPU in your cluster. Normally, Spark tries to set the number of partitions automatically based on your cluster. However, you can also set it manually by passing it as a second parameter to parallelize (e.g. sc.parallelize(data, 10)). Note: some places in the code use the term slices (a synonym for partitions) to maintain backward compatibility.</p>
<h1 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h1><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams.Data can be ingested from many sources like Kafka, Flume, Kinesis, or TCP sockets,. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.</p>
<p>Internally, it works as follows. Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.</p>
<p>Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis, or by applying high-level operations on other DStreams. Internally, a DStream is represented as a sequence of RDDs.</p>
<h1 id="DStreams"><a href="#DStreams" class="headerlink" title="DStreams"></a>DStreams</h1><p>Discretized Stream or DStream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. Internally, a DStream is represented by a continuous series of RDDs, which is Spark’s abstraction of an immutable, distributed dataset (see Spark Programming Guide for more details). Each RDD in a DStream contains data from a certain interval, as shown in the following figure.</p>
<h1 id="Window-Operations"><a href="#Window-Operations" class="headerlink" title="Window Operations"></a>Window Operations</h1><p>Spark Streaming also provides windowed computations, which allow you to apply transformations over a sliding window of data. The following figure illustrates this sliding window.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;DStreamDAG&quot;&gt;&lt;a href=&quot;#DStreamDAG&quot; class=&quot;headerlink&quot; title=&quot;DStreamDAG&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/lw-lin/CoolplaySpark/blob/ma
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>linux常用命令</title>
    <link href="http://yoursite.com/2017/08/16/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    <id>http://yoursite.com/2017/08/16/linux常用命令/</id>
    <published>2017-08-16T03:53:25.000Z</published>
    <updated>2017-09-01T07:28:01.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="查看系统版本"><a href="#查看系统版本" class="headerlink" title="查看系统版本"></a>查看系统版本</h1><ul>
<li>Redhat系<br>cat /etc/redhat-release     –&gt;     <code>CentOS Linux release 7.3.1611 (Core)</code></li>
</ul>
<ul>
<li>查看系统内核版本<br>uname -a<br>Linux localhost.localdomain 3.10.0-514.16.1.el7.x86_64 #1 SMP Wed Apr 12 15:04:24 UTC 2017 x86_64 x86_64 x86_64 GNU/Linu</li>
</ul>
<h1 id="查看端口占用状态"><a href="#查看端口占用状态" class="headerlink" title="查看端口占用状态"></a>查看端口占用状态</h1><p>Netstat 命令用于显示各种网络相关信息，如网络连接，路由表，接口状态 (Interface Statistics)，masquerade 连接，多播成员 (Multicast Memberships) 等等。</p>
<p>Active Internet connections，称为有源TCP连接，其中”Recv-Q”和”Send-Q”指%0A的是接收队列和发送队列。这些数字一般都应该是0。如果不是则表示软件包正在队列中堆积。这种情况只能在非常少的情况见到。</p>
<ol>
<li>-a (all)显示所有选项，默认不显示LISTEN相关</li>
<li>-t (tcp)仅显示tcp相关选项</li>
<li>-u (udp)仅显示udp相关选项</li>
<li>-n 拒绝显示别名，能显示数字的全部转化成数字</li>
<li>-l 仅列出有在 Listen (监听) 的服務状态</li>
<li>-p 显示建立相关链接的程序名</li>
<li>-r 显示路由信息，路由表</li>
<li>-e 显示扩展信息，例如uid等</li>
<li>-s 按各个协议进行统计</li>
<li>-c 每隔一个固定时间，执行该netstat命令。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">-- 查询指定端口运行的进程</div><div class="line"></div><div class="line">[root@localhost ~]# netstat -anp | grep &apos;:8080&apos;</div><div class="line">tcp6       0      0 :::8080                 :::*                    LISTEN      73699/java</div><div class="line"></div><div class="line">-- 查询所有tcp链接端口</div><div class="line"></div><div class="line">[root@localhost ~]# netstat -nat</div><div class="line">Active Internet connections (servers and established)</div><div class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State</div><div class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN</div><div class="line">tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN</div></pre></td></tr></table></figure>
<h1 id="Netcat-网络工具"><a href="#Netcat-网络工具" class="headerlink" title="Netcat 网络工具"></a><a href="https://www.oschina.net/translate/linux-netcat-command" target="_blank" rel="external">Netcat</a> 网络工具</h1><p>netcat是网络工具中的瑞士军刀，它能通过TCP和UDP在网络中读写数据。netcat所做的就是在两台电脑之间建立链接并返回两个数据流，在这之后所能做的事就看你的想像力了。你能建立一个服务器，传输文件，与朋友聊天，传输流媒体或者用它作为其它协议的独立客户端。</p>
<ol>
<li>-z 告诉netcat使用0 IO,连接成功后立即关闭连接， 不进行数据交换</li>
<li>-v 指使用冗余选项</li>
<li>-n 告诉netcat 不要使用DNS反向查询IP地址的域名</li>
</ol>
<ul>
<li>端口扫描<br>端口扫描经常被系统管理员和黑客用来发现在一些机器上开放的端口，帮助他们识别系统中的漏洞。<br><code>$nc -z -v -n 172.31.100.7 21-25</code><br>一旦你发现开放的端口，你可以容易的使用netcat 连接服务抓取他们的banner。<br><code>$nc -v 172.31.100.7 21</code></li>
</ul>
<h2 id="Chat-Server"><a href="#Chat-Server" class="headerlink" title="Chat Server"></a>Chat Server</h2><p>假如你想和你的朋友聊聊，有很多的软件和信息服务可以供你使用。但是，如果你没有这么奢侈的配置，比如你在计算机实验室，所有的对外的连接都是被限制的，你怎样和整天坐在隔壁房间的朋友沟通那？不要郁闷了，netcat提供了这样一种方法，你只需要创建一个Chat服务器，一个预先确定好的端口，这样子他就可以联系到你了。<br>Server(A) <code>$nc -l 1567</code><br>Client(B) <code>$nc 172.31.100.7 1567</code><br>netcat 命令在1567端口启动了一个tcp 服务器，所有的标准输出和输入会输出到该端口。输出和输入都在此shell中展示, 不管你在机器B上键入什么都会出现在机器A上。</p>
<h2 id="文件传输"><a href="#文件传输" class="headerlink" title="文件传输"></a>文件传输</h2><p>大部分时间中，我们都在试图通过网络或者其他工具传输文件。有很多种方法，比如FTP,SCP,SMB等等，但是当你只是需要临时或者一次传输文件，真的值得浪费时间来安装配置一个软件到你的机器上嘛。假设，你想要传一个文件file.txt 从A 到B。A或者B都可以作为服务器或者客户端，以下，让A作为服务器，B为客户端。<br>Server <code>$nc -l 1567 &lt; file.txt</code><br>Client <code>$nc -n 172.31.100.7 1567 &gt; file.txt</code></p>
<p>这里我们创建了一个服务器在A上并且重定向netcat的输入为文件file.txt，那么当任何成功连接到该端口，netcat会发送file的文件内容。<br>在客户端我们重定向输出到file.txt，当B连接到A，A发送文件内容，B保存文件内容到file.txt.</p>
<p>没有必要创建文件源作为Server，我们也可以相反的方法使用。像下面的我们发送文件从B到A，但是服务器创建在A上，这次我们仅需要重定向netcat的输出并且重定向B的输入文件。</p>
<h1 id="expect"><a href="#expect" class="headerlink" title="expect"></a><a href="http://www.cnblogs.com/lzrabbit/p/4298794.html" target="_blank" rel="external">expect</a></h1><p>Expect是一个用来处理交互的命令。借助Expect，我们可以将交互过程写在一个脚本上，使之自动化完成。形象的说，ssh登录，ftp登录等都符合交互的定义。下文我们首先提出一个问题，然后介绍基础知四个命令，最后提出解决方法。</p>
<p>Expect中最关键的四个命令是send,expect,spawn,interact。</p>
<ol>
<li>send：用于向进程发送字符串</li>
<li>expect：从进程接收字符串</li>
<li>spawn：启动新的进程</li>
<li>interact：允许用户交互</li>
</ol>
<p>匹配到hi,hello,bye任意一个字符串时，执行相应的输出。等同于如下写法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/expect -f</div><div class="line">set timeout -1</div><div class="line">spawn ftp ftp.test.com      //打开新的进程，该进程用户连接远程ftp服务器</div><div class="line">expect &#123;</div><div class="line">&quot;hi&quot; &#123; send &quot;You said hi\n&quot;&#125;</div><div class="line">&quot;hello&quot; &#123; send &quot;Hello yourself\n&quot;&#125;</div><div class="line">&quot;bye&quot; &#123; send &quot;That was unexpected\n&quot;&#125;</div><div class="line">&#125;</div><div class="line">interact</div></pre></td></tr></table></figure>
<h1 id="Find"><a href="#Find" class="headerlink" title="Find"></a><a href="http://man.linuxde.net/find" target="_blank" rel="external">Find</a></h1>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;查看系统版本&quot;&gt;&lt;a href=&quot;#查看系统版本&quot; class=&quot;headerlink&quot; title=&quot;查看系统版本&quot;&gt;&lt;/a&gt;查看系统版本&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Redhat系&lt;br&gt;cat /etc/redhat-release     –&amp;gt;    
    
    </summary>
    
      <category term="linux" scheme="http://yoursite.com/categories/linux/"/>
    
    
  </entry>
  
  <entry>
    <title>代码风格要素</title>
    <link href="http://yoursite.com/2017/08/13/%E4%BB%A3%E7%A0%81%E9%A3%8E%E6%A0%BC%E8%A6%81%E7%B4%A0/"/>
    <id>http://yoursite.com/2017/08/13/代码风格要素/</id>
    <published>2017-08-13T09:26:29.000Z</published>
    <updated>2017-08-13T10:20:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>The elements of programming style</p>
<ol>
<li>Write clearly - don’t be too clever<br> 代码写清楚，别耍小聪明。</li>
</ol>
<h2 id="Expression-代码表达"><a href="#Expression-代码表达" class="headerlink" title="Expression 代码表达"></a>Expression 代码表达</h2><ol>
<li>Say what you mean, simply and directly.<br> 想干什么，写的简单点，直接点。</li>
<li>Use library functions.<br> 有库函数，就用库函数。</li>
<li>Avoid temporary variables.<br> 避免临时变量？？？</li>
<li>Write clearly - don’t sacrifice clarity for ‘efficiency.’<br> 不要因为性能优化而牺牲程序的简洁，清晰性。</li>
<li>Let the machine do the dirty work.<br> 让程序做重复的工作。</li>
<li>Replace repetitive expressions by calls to a common function.<br> 不要重复写代码。</li>
<li>Parenthesize to avoid ambiguity.<br> 括号来避免歧义。</li>
<li>Choose variable names that won’t be confused.<br> 变量名要有意义。</li>
<li>Avoid unnecessary branches.<br> 避免不必要的分支。</li>
<li>Use the good features of a language; avoid the bad ones.<br> 用语言好的特使。</li>
</ol>
<h2 id="Control-Structure-控制结构"><a href="#Control-Structure-控制结构" class="headerlink" title="Control Structure 控制结构"></a>Control Structure 控制结构</h2><ol>
<li>Make your programs read from top to bottom.<br> 确保你的程序循序渐进。</li>
<li>Use IF, ELSE IF, ELSE IF, ELSE to implement multi-way branches.<br>   用 if else then else 实现多分支逻辑。</li>
<li>Use the fundamental control flow constructs.<br> 用基本的条件控制结构</li>
<li>Write first in an easy-to-understand pseudo-language; then translate into whatever language you have to use.<br> 先用伪代码，然后翻译成你使用的语言。</li>
<li>Avoid THEN-IF and null ELSE.<br> 避免then if and null else</li>
<li>Avoid ELSE GOTO and ELSE RETURN.<br> 避免goto语句。</li>
<li>Follow each decision as closely as possible with its associated action.<br> 语言逻辑块要尽可能的在一块。不要太分散。</li>
<li>Use data arrays to avoid repetitive control sequences.<br> 使用数组避免重复。</li>
<li>Choose a data representation that makes the program simple.<br> 选择让程序更简洁的数据表达形式。</li>
<li>Don’t stop with your first draft.<br> 先用伪代码，然后翻译成你使用的语言。</li>
</ol>
<h2 id="Program-Structure-代码结构"><a href="#Program-Structure-代码结构" class="headerlink" title="Program Structure 代码结构"></a>Program Structure 代码结构</h2><ol>
<li>Modularize. Use subroutines.<br> 模块化，用子程序。</li>
<li>Make the coupling between modules visible.<br> 模块间的耦合要可见，清晰。</li>
<li>Each module should do one thing well.<br> 每个模块只做一件事情。</li>
<li>Make sure every module hides something.<br> 每个模块保证隐藏细节</li>
<li>Let the data structure the program.<br> 编程之前，先选择好的数据结构。</li>
<li>Don’t patch bad code - rewrite it.<br> 不要给垃圾代码打补丁，直接从写。</li>
<li>Write and test a big program in small pieces.<br> 把大的程序分成一小片一小片，写和测试。</li>
<li>Use recursive procedures for recursively-defined data structures.<br> 使用递归程序来处理递归定义的数据结构</li>
</ol>
<h2 id="Input-and-Output-输入输出"><a href="#Input-and-Output-输入输出" class="headerlink" title="Input and Output 输入输出"></a>Input and Output 输入输出</h2><ol>
<li>Test input for validity and plausibility.<br> 正确和错误的输入都要测试。</li>
<li>Make sure input cannot violate the limits of the program.<br> 确保输入不会超出程序的限制。</li>
<li>Terminate input by end-of-file or marker, not by count.<br> 依靠文件结束符号来表示读入结束，而不是记文件的字节数。</li>
<li>Identify bad input; recover if possible.<br> 识别错误的输入，尽可能的纠正。</li>
<li>Treat end of file conditions in a uniform manner.<br> 统一的方法来处理文件结束</li>
<li>Make input easy to prepare and output self-explanatory.<br> 让输入数据很容易构造出来，让输出数据不言自明。</li>
<li>Use uniform input formats.<br> 使用统一的输入格式。</li>
<li>Make input easy to proofread.<br> 让输入更容易校对。</li>
<li>Use free-form input when possible.<br> 如果可能，提供更自由的输入格式。</li>
<li>Use self-identifying input. Allow defaults. Echo both on output.<br> 使用输入提示，允许使用默认值，应在开始运行时显出出来。</li>
<li><p>Localize input and output in subroutines.<br> 输入输出都放在自程序中</p>
<h2 id="Common-Blunders"><a href="#Common-Blunders" class="headerlink" title="Common Blunders"></a>Common Blunders</h2></li>
<li><p>Make sure all variables are initialized before use.<br> 确保所有变量在使用前都初始化。</p>
</li>
<li>Don’t stop at one bug.<br> 不要因为一个bug而停止不前。</li>
<li>Use debugging compilers.<br> 打开编译程序的调试选项。</li>
<li>Initialize constants with DATA statements or INITIAL attributes; initialize variables with executable code.<br> 常量结构用数据声明初始化，变量结构用执行代码初始化。</li>
<li>Watch out for off-by-one errors.<br> 小心 off-by-one错误。</li>
<li>Take care to branch the right way on equality.<br> 在判断进入分支的相等表达式的时候，要注意。</li>
<li>Avoid multiple exits from loops.<br> 当循环中有多个点可以跳出循环的时候，要格外小心。</li>
<li>Make sure your code ‘does nothing’ gracefully.<br> 如果什么也不做，那么也要优雅的表现出这个意思。</li>
<li>Test programs at their boundary values.<br> 用边界值去测试你的程序</li>
<li>Program defensively.<br> 防御性编程。</li>
<li>10.0 times 0.1 is hardly ever 1.0.<br> 10.0 乘以 0.1 不一定永远是 1.0</li>
<li>Don’t compare floating point numbers just for equality.<br> 不要直接判断浮点类型</li>
</ol>
<h2 id="Efficiency-and-Instrumentation"><a href="#Efficiency-and-Instrumentation" class="headerlink" title="Efficiency and Instrumentation"></a>Efficiency and Instrumentation</h2><ol>
<li>Make it right before you make it faster.<br> 优化之前，先让代码正确。</li>
<li>Keep it right when you make it faster.<br> 优化之前，先保证程序正确。</li>
<li>Make it clear before you make it faster.<br> 优化之前，先让程序简单明了。</li>
<li>Don’t sacrifice clarity for small gains in ‘efficiency.’<br> 不要因为一点的性能去牺牲程序的简洁性</li>
<li>Let your compiler do the simple optimizations.<br> 让编译器做一些简单的优化</li>
<li>Don’t strain to re-use code; reorganize instead.<br> 不要过分追求重用代码，下次用的时候再重构也不迟。</li>
<li>Make sure special cases are truly special.<br>确保特殊情况真的特殊。</li>
<li>Keep it simple to make it faster.<br>保持简单来获得性能。</li>
<li>Don’t diddle code to make it faster - find a better algorithm.<br> 不要死磕代码来提高性能， 尝试着用一个好的算法或者数据结构。</li>
<li>Instrument your programs. Measure before making ‘efficiency’ changes.<br>  用工具分析一下你的代码，在做优化之前先测评一下。</li>
</ol>
<h2 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h2><ol>
<li>Make sure comments and code agree.<br>注释和代码保持一致。</li>
<li>Don’t just echo the code with comments - make every comment count.<br>不要重复注释，每一个注释都应该有意义。</li>
<li>Don’t comment bad code - rewrite it.<br>不要尝试修改垃圾代码，直接重写它。</li>
<li>Use variable names that mean something.<br>不要瞎起变量名。</li>
<li>Use statement labels that mean something.<br>声明的语句的标签名字要有意义。</li>
<li>Format a program to help the reader understand it.<br>提交代码之前格式化</li>
<li>Indent to show the logical structure of a program.<br>不同的逻辑结构用缩紧来区分</li>
<li>Document your data layouts.<br>为你的数据布局写一个文档</li>
<li>Don’t over-comment.<br>不要过度注释</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The elements of programming style&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Write clearly - don’t be too clever&lt;br&gt; 代码写清楚，别耍小聪明。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;Expression-代码表达&quot;&gt;&lt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>kafka</title>
    <link href="http://yoursite.com/2017/07/30/kafka/"/>
    <id>http://yoursite.com/2017/07/30/kafka/</id>
    <published>2017-07-29T16:26:53.000Z</published>
    <updated>2017-08-30T12:41:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>Apache Kafka is a publish/subscribe messaging system designed to solve this prob‐ lem. It is often described as a “distributed commit log”. A filesystem or database com‐ mit log is designed to provide a durable record of all transactions so that they can be replayed to consistently build the state of a system. Similarly, data within Kafka is stored durably, in order, and can be read deterministically. In addition, the data can be distributed within the system to provide additional protections against failures, as well as significant opportunities for scaling performance.</p>
<h1 id="消息和批处理"><a href="#消息和批处理" class="headerlink" title="消息和批处理"></a>消息和批处理</h1><p>The unit of data within Kafka is called a message. If you are approaching Kafka from a database background, you can think of this as similar to a row or a record. A message is simply an array of bytes, as far as Kafka is concerned, so the data contained within it does not have a specific format or meaning to Kafka. Messages can have an optional bit of metadata which is referred to as a key. The key is also a byte array, and as with the message, has no specific meaning to Kafka. Keys are used when messages are to be written to partitions in a more controlled manner. The simplest such scheme is to treat partitions as a hash ring, and assure that messages with the same key are always written to the same partition. Usage of keys is discussed more thoroughly in Chap‐ ter 3.<br>For efficiency, messages are written into Kafka in batches. A batch is just a collection of messages, all of which are being produced to the same topic and partition. An indi‐ vidual round trip across the network for each message would result in excessive over‐ head, and collecting messages together into a batch reduces this. This, of course, presents a tradeoff between latency and throughput: the larger the batches, the more messages that can be handled per unit of time, but the longer it takes an individual message to propagate. Batches are also typically compressed, which provides for more efficient data transfer and storage at the cost of some processing power.</p>
<h1 id="Topic-amp-Partitions"><a href="#Topic-amp-Partitions" class="headerlink" title="Topic &amp; Partitions"></a>Topic &amp; Partitions</h1><p>Messages in Kafka are categorized into topics. The closest analogy for a topic is a data‐ base table, or a folder in a filesystem.<br>Topics are additionally broken down into a number of partitions.<br> Going back to the “commit log” description, a partition is a sin‐ gle log. Messages are written to it in an append-only fashion, and are read in order from beginning to end. . Note that as a topic generally has multiple partitions, there is no guarantee of time-ordering of messages across the entire topic, just within a single partition. Figure 1-5 shows a topic with 4 partitions, with writes being appended to the end of each one. Partitions are also the way that Kafka provides redundancy and scalability. Each partition can be hosted on a different server, which means that a sin‐ gle topic can be scaled horizontally across multiple servers to provide for perfor‐ mance far beyond the ability of a single server.</p>
<h1 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h1><p>the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly. In some cases, the pro‐ ducer will direct messages to specific partitions. This is typically done using the mes‐ sage key and a partitioner that will generate a hash of the key and map it to a specific partition. </p>
<h1 id="Consumers"><a href="#Consumers" class="headerlink" title="Consumers"></a>Consumers</h1><p>The consumer keeps track of which messages it has already consumed by keeping track of the o set of messages. The offset is another bit of metadata, an integer value that continually increases, that Kafka adds to each message as it is produced. Each message within a given partition has a unique offset. By storing the offset of the last consumed message for each parti‐ tion, either in Zookeeper or in Kafka itself, a consumer can stop and restart without losing its place.</p>
<h1 id="Consumer-Group"><a href="#Consumer-Group" class="headerlink" title="Consumer Group"></a>Consumer Group</h1><p>Consumers work as part of a consumer group. This is one or more consumers that work together to consume a topic. The group assures that each partition is only con‐ sumed by one member.</p>
<h1 id="Brokers-amp-Clusters"><a href="#Brokers-amp-Clusters" class="headerlink" title="Brokers &amp; Clusters"></a>Brokers &amp; Clusters</h1><p>A single Kafka server is called a broker. The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the mes‐ sages that have been committed to disk. Depending on the specific hardware and its performance characteristics, a single broker can easily handle thousands of partitions and millions of messages per second.</p>
<p>Kafka brokers are designed to operate as part of a cluster. Within a cluster of brokers, one will also function as the cluster controller (elected automatically from the live members of the cluster). The controller is responsible for administrative operations, including assigning partitions to brokers and monitoring for broker failures.</p>
<p> A parti‐ tion is owned by a single broker in the cluster, and that broker is called the leader for the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated (as in Figure 1-7). This provides redundancy of messages in the partition, such that another broker can take over leadership if there is a broker failure. However, all consumers and producers operating on that partition must con‐ nect to the leader. Cluster operations, including partition replication, are covered in detail in Chapter 6.</p>
<h1 id="Why-kafka"><a href="#Why-kafka" class="headerlink" title="Why kafka"></a>Why kafka</h1><h2 id="Multiple-Producers"><a href="#Multiple-Producers" class="headerlink" title="Multiple Producers"></a>Multiple Producers</h2><p>Kafka is able to seamlessly handle multiple producers, whether those clients are using many topics or the same topic. This makes the system ideal for aggregating data from many front end systems and providing the data in a consistent format. For example, a site that serves content to users via a number of microservices can have a single topic for page views which all services can write to using a common format. Consumer applications can then received one unified view of page views for the site without having to coordinate the multiple producer streams.</p>
<h2 id="Multiple-Consumers"><a href="#Multiple-Consumers" class="headerlink" title="Multiple Consumers"></a>Multiple Consumers</h2><p>In addition to multiple consumers, Kafka is designed for multiple consumers to read any single stream of messages without interfering with each other. This is in opposi‐ tion to many queuing systems where once a message is consumed by one client, it is not available to any other client. At the same time, multiple Kafka consumers can choose to operate as part of a group and share a stream, assuring that the entire group processes a given message only once.</p>
<h1 id="Disk-based-Retention"><a href="#Disk-based-Retention" class="headerlink" title="Disk-based Retention"></a>Disk-based Retention</h1><p>Not only can Kafka handle multiple consumers, but durable message retention means that consumers do not always need to work in real time. Messages are committed to disk, and will be stored with configurable retention rules. These options can be selected on a per-topic basis, allowing for different streams of messages to have differ‐ ent amounts of retention depending on what the consumer needs are. Durable reten‐ tion means that if a consumer falls behind, either due to slow processing or a burst in traffic, there is no danger of losing data. It also means that maintenance can be per‐ formed on consumers, taking applications offline for a short period of time, with no concern about messages backing up on the producer or getting lost. The consumers can just resume processing where they stopped.</p>
<h1 id="Scalable"><a href="#Scalable" class="headerlink" title="Scalable"></a>Scalable</h1><p>Flexible scalability has been designed into Kafka from the start, allowing for the abil‐ ity to easily handle any amount of data. Users can start with a single broker as a proof of concept, expand to a small development cluster of 3 brokers, and move into pro‐ duction with a larger cluster of tens, or even hundreds, of brokers that grows over time as the data scales up. Expansions can be performed while the cluster is online, with no impact to the availability of the system as a whole. This also means that a cluster of multiple brokers can handle the failure of an individual broker and con‐ tinue servicing clients. Clusters that need to tolerate more simultaneous failures can be configured with higher replication factors. Replication is discussed in more detail in Chapter 6.</p>
<h1 id="High-Performance"><a href="#High-Performance" class="headerlink" title="High Performance"></a>High Performance</h1><p>All of these features come together to make Apache Kafka a publish/subscribe mes‐ saging system with excellent performance characteristics under high load. Producers, consumers, and brokers can all be scaled out to handle very large message streams with ease. This can be done while still providing sub-second message latency from producing a message to availability to consumers.</p>
<h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><p>Apache Kafka uses Zookeeper to store metadata information about the Kafka cluster, as well as consumer client details. While it is possible to run a Zookeeper server using scripts contained within the Kafka distribution, it is trivial to install a full version of Zookeeper from the distribution.<br><img src="/2017/07/30/kafka/kafka_zookeeper.png" alt="kafka_zookeeper"></p>
<h1 id="topic-Defaults"><a href="#topic-Defaults" class="headerlink" title="topic Defaults"></a>topic Defaults</h1><p>The Kafka server configuration specifies many default configurations for topics that are created. Several of these parameters, including partition counts and message retention, can be set per-topic using the administrative tools (covered in Chapter 9). The defaults in the server configuration should be set to baseline values that are appropriate for the majority of the topics in the cluster.</p>
<blockquote>
<p>In previous versions of Kafka, it was possible to specify per-topic overrides for these configurations in the broker configuration using parameters named log.reten tion.hours.per.topic, log.retention.bytes.per.topic, and log.seg ment.bytes.per.topic. These parameters are no longer supported, and overrides must be specified using the administrative tools.</p>
</blockquote>
<h1 id="多集群"><a href="#多集群" class="headerlink" title="多集群"></a>多集群</h1><ul>
<li>分离不同类型的数据</li>
<li>分离不同的安全策略</li>
<li>多个数据中心（灾难恢复)</li>
</ul>
<p>当系统需要和多个数据中心交互的时候，通常情况是在不同的数据中心之间复制消息，这样应用程序便可以统一的访问用户的动态数据（数据分布在不同的数据中心），或者是监控程序可以从不同的数据中心收集数据到分析系统和报警系统所在的服务器。 之所以这样，是因为消息的冗余机制是在一个kafka集群的基础的简历的，不能在多个集群之间冗余。</p>
<p>kafka Mirror Maker 实现图:</p>
<p><img src="/2017/07/30/kafka/multiple_datacenter_arhitecture.png" alt="multiple datacenter architecture"></p>
<h1 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h1><h2 id="offset"><a href="#offset" class="headerlink" title="offset"></a>offset</h2><ul>
<li>每一个分区对应消息的offset是唯一的。 partition + topic + groupId</li>
<li>我们称 在topic分区中更新当前位置的操作叫做 commit。</li>
<li>offset 可以存储在zookeeper或者kafka broker上</li>
<li>consumer作为consumer group的一部分去消费一个topic上的消息。 group保证同一个topic上一个分区的消息只能被一个consumer消费，而不是group里面所有的consumer。<br><img src="/2017/07/30/kafka/topic_paertitions.png" alt="topic partitions"></li>
</ul>
<h2 id="consumer怎样存储offset"><a href="#consumer怎样存储offset" class="headerlink" title="consumer怎样存储offset"></a>consumer怎样存储offset</h2><ul>
<li>对于不同的分区，consumer发送commited offset到 _consumer_offsets Topic.只要所有的consumer都在运行，就没有问题。但是当有新的consumer加入或推出的时候就会触发再平衡，再平衡之后，每一个consumer被分配新的分区这个时候它怎么知道当前分区读到哪一个位置了？ 这个时候consumer就会通过读取每个分区对应的最新的offset来继续服务。</li>
<li>如果commited offset 小于最后一次客户端提交offset，那么这中间的消息机会重复消费。反之亦然。</li>
</ul>
<p>kafka consumer api 提供了多种不同的方式去提交offset。</p>
<ol>
<li><p>自动提交， enable.auto.commit = true，那么auto.commit.interval.ms 毫秒之后就会自动提交， 自动提交又 poll循环实现，每一次你拉取消息，consumer就会检查是否需要提交。如果是，则提交。</p>
<blockquote>
<p>如果配置自动提交，在提交间隔之间，一个consumer死掉或者新的加入，触发在平衡，之后所有的consumer就会读取分配分区的offset进行消费，这个时候在那个间隔之间的消息就会重复消费，你可以配置自动提交时间间隔，但是这种情况时不可避免的，这要看你怎么权衡消费系统。</p>
</blockquote>
</li>
<li><p>手动提交。 auto.commit.offset = false，手动提交可以尽可能的避免丢失消息，或者在再平衡过程之后重复消费消息。</p>
<ul>
<li>最简单的事 commitSync()方法，它会提交最新的上一次poll操作返回的offset，提交成功则返回，失败则抛出异常。</li>
<li>切记调用commitSync()之前，处理完poll回来的所有消息。</li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">while (true) &#123;        ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);        for (ConsumerRecord&lt;String, String&gt; record : records)        &#123;            System.out.println(&quot;topic = %s, partition = %s, offset = %d, customer =    %s, country = %s\n&quot;,                                record.topic(), record.partition(), record.off-    set(), record.key(), record.value());&#125; try &#123;          consumer.commitSync();        &#125; catch (CommitFailedException e) &#123;            log.error(&quot;commit failed&quot;, e)        &#125;&#125;</div></pre></td></tr></table></figure>
<ol>
<li><p>异步提交，同样的我们可以异步的提交offset通过调用commitAsync(),这样提交操作就不会阻塞在等待broker响应提交的请求上，增加了系统的吞吐量。</p>
<ul>
<li>异步的提交会重试提交，当发生提交错误的时候直至成功。除非碰到那种不可修复的错误。之所以停止重试，是因为commitAsync()收到回复的时候也许之后的commit成功了，想象一下，如果发送提交offset=2000，由于暂时的原因失败了，这个时候我们处理另外的请求offset=3000，如果提交2000请求发生在提交3000请求之后，就会导致更多的消息重读。 因此保证异步提交offset的顺序非常重要。通常情况下对于commitAsync方法我们可以传入回调接口去打印失败的日志。</li>
</ul>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">while (true) &#123;        ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);        for (ConsumerRecord&lt;String, String&gt; record : records) &#123;            System.out.println(&quot;topic = %s, partition = %s, offset = %d, customer =    %s, country = %s\n&quot;,                                record.topic(), record.partition(), record.off-    set(), record.key(), record.value());        &#125;        consumer.commitAsync(new OffsetCommitCallback() &#123;            public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,    Exception exception) &#123;                if (e != null)                    log.error(&quot;Commit failed for offsets &#123;&#125;&quot;, offsets, e);&#125; &#125;);&#125;</div></pre></td></tr></table></figure>
<pre><code>* 我们应该在最后consumer close方法之前调用 commitSync方法保证commit真正的成功。
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"> try &#123;        while (true) &#123;            ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);            for (ConsumerRecord&lt;String, String&gt; record : records) &#123;                System.out.println(&quot;topic = %s, partition = %s, offset = %d, cus-    tomer = %s, country = %s\n&quot;,                                  record.topic(), record.partition(), record.off-    set(), record.key(), record.value());&#125;            consumer.commitAsync();        &#125;    &#125; catch (Exception e) &#123;        log.error(&quot;Unexpected error&quot;, e);    &#125; finally &#123;        try &#123;            consumer.commitSync();        &#125; finally &#123;            consumer.close();        &#125;&#125;</div></pre></td></tr></table></figure>
<ol>
<li>提交指定的offset，如果poll返回了特别大的消息，并且你象提交offset在处理消息的过程中，去避免由于处理失败导致处理过的消息的重读。你可以提交指定分区的offset。</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">private</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets;    <span class="keyword">int</span> count = <span class="number">0</span>;....    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;        ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records)        &#123;            System.out.println(<span class="string">"topic = %s, partition = %s, offset = %d, customer =    %s, country = %s\n"</span>,                                record.topic(), record.partition(), record.off-    set(), record.key(), record.value());            currentOffsets.put(<span class="keyword">new</span> TopicPartition(record.topic(), record.parti-    tion()),                               record.offset());            <span class="keyword">if</span> (count % <span class="number">1000</span> == <span class="number">0</span>)                consumer.commitAsync(currentOffsets);            count++;&#125; </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<hr>
<ul>
<li>提高消费横向扩展的主要方式就是在consumer group 中增加consumer，直至等于该topic<br>partitions的个数。这样的话consumer就可以干一些i/o，费时间的计算等工作。 这也是为什么topic要配置多个partitions的原因，就是为了消费的横向扩展。</li>
<li>一个topic被不同应用系统消费也是非常常见的，也是kafka设计的主要驱动之一。方法就是不同的应用创建不同的consumer group，这样每一个consumer group就可以得到topic全部分区的消息，而不是其中几个分区的消息。同时kafka也可以保证横向扩展的consumer group不会影响系统的效率。</li>
</ul>
<p><img src="/2017/07/30/kafka/kafka_group.png" alt="kafka group"></p>
<h2 id="topic-partition-rebalance"><a href="#topic-partition-rebalance" class="headerlink" title="topic partition rebalance"></a>topic partition rebalance</h2><ul>
<li>你可以在consumer subscibe broker的时候指定rebalance listeners</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">private Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets;    private class HandleRebalance implements ConsumerRebalanceListener &#123;        public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123;        &#125;        public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123;            consumer.commitSync(currentOffsets);&#125;&#125;try &#123;    consumer.subscribe(topics, new HandleRebalance());    while (true) &#123;        ConsumerRecords&lt;String, String&gt; records = consumer.poll(100);        for (ConsumerRecord&lt;String, String&gt; record : records)        &#123;            System.out.println(&quot;topic = %s, partition = %s, offset = %d, cus-tomer = %s, country = %s\n&quot;,                                record.topic(), record.partition(), record.off-set(), record.key(), record.value());             currentOffsets.put(new TopicPartition(record.topic(), record.partition()),record.offset());&#125;        consumer.commitAsync(currentOffsets);    &#125;&#125; catch (WakeupException e) &#123;    // ignore, we&apos;re closing&#125; catch (Exception e) &#123;    log.error(&quot;Unexpected error&quot;, e);&#125; finally &#123;    try &#123;        consumer.commitSync(currentOffsets);    &#125; finally &#123;        consumer.close();    &#125;&#125;</div></pre></td></tr></table></figure>
<ul>
<li>当在一个consumer group中增加或减少consumer的时候都会出发 partition的rebalance。</li>
<li>再平衡保证了 consumer的高可用和横向扩展性。</li>
<li>再平衡并不赞成经常处罚，因为在再平衡的过程中consumer无法继续消费消息，导致效率的降低。</li>
</ul>
<ul>
<li>consumers维持和consumer group的关系，以及和partitions的关联关系是通过发送心跳请求到broker来实现的。(Group Coordinator，对于不同的consumer group，broker可以是不同的)，只要定时收到来自consumer的心跳，broker就认定它是活动的。</li>
<li>实际上consumer通过从broker拉取消息的同时发送心跳的，如果consumer停止发送心跳一定的时间，consumer和broker之间的会话就会过期，group coordinator就是认定这个consumer已经死亡从而触发再平衡。</li>
<li>当一个consumer加入到group中时会发送 JoinGroup 请求到group coordinator，第一个加入的consumer被认定为group leader， leader通过group coordinator查询得到group中所有的consumer的信息，然后负责分配不同consumer对应的partition，kafka有两个内置的分区分配策略，这个可以在配置文件中配置。 当分配好不同的consumer对应的partition之后，leader发送分配的信息给 group coordinator，通过它发送到所有的consumer上，每一个consumer将只会看到自己的分区消息。 当再平衡触发的时候以上的过程回重复执行。</li>
</ul>
<h2 id="如何保证消费消息exactly-once"><a href="#如何保证消费消息exactly-once" class="headerlink" title="如何保证消费消息exactly once"></a>如何保证消费消息exactly once</h2><ul>
<li>use ConsumerRebanlanceListener and seek()方法</li>
<li>存储offset和处理消息在一个事务中</li>
<li>存储offset和消息处理到同一个地方， 消息处理存储到db，那么offset也应该存储到db。</li>
</ul>
<h2 id="topic-partition-replication"><a href="#topic-partition-replication" class="headerlink" title="topic partition replication"></a>topic partition replication</h2><p><img src="/2017/07/30/kafka/replication_of_partitions_in_cluster.png" alt="replication of partition in a cluster"></p>
<h1 id="topic"><a href="#topic" class="headerlink" title="topic"></a>topic</h1><ul>
<li>每一种topic都可以根据consumer的需求配置它的存储策略。</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Apache Kafka is a publish/subscribe messaging system designed to solve this prob‐ lem. It is often described as a “distributed commit log
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>大数据术语</title>
    <link href="http://yoursite.com/2017/07/29/%E6%9C%AF%E8%AF%AD/"/>
    <id>http://yoursite.com/2017/07/29/术语/</id>
    <published>2017-07-29T12:54:29.000Z</published>
    <updated>2017-07-29T13:07:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="failover"><a href="#failover" class="headerlink" title="failover"></a>failover</h1><blockquote>
<p>失效备援, 为系统备援能力的一种，当系统中其中一项设备失效而无法运作时，另一项设备即可自动接手原失效系统所执行的工作</p>
</blockquote>
<h1 id="failback"><a href="#failback" class="headerlink" title="failback"></a>failback</h1><blockquote>
<p>故障恢复, 是当系统主服务恢复之后，从备用服务切换到主服务的过程，这个过程相对来说比较长，因为它要把在主服务失败期间，备用服务生成的数据拷贝回主服务之后才能切换成功。</p>
</blockquote>
<h1 id="fault-tolerant"><a href="#fault-tolerant" class="headerlink" title="fault tolerant"></a>fault tolerant</h1><blockquote>
<p>故障容限, 系统对于未预期的硬件或者软件异常能够很好的作出响应， 具体分为硬件层次的故障容错，和<br>软件层次的故障容错。 硬件容错是在发生故障的时候（电力中断）有备用电源继续提供服务。 软件容错<br>一般是可以通过冗余（冗余的磁盘、计算机以及数据中心）来提供容错，但是事务 使得构建容错的软件应用程序成为可能。</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;failover&quot;&gt;&lt;a href=&quot;#failover&quot; class=&quot;headerlink&quot; title=&quot;failover&quot;&gt;&lt;/a&gt;failover&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;失效备援, 为系统备援能力的一种，当系统中其中一项设备失效而无法运
    
    </summary>
    
      <category term="bigdata" scheme="http://yoursite.com/categories/bigdata/"/>
    
    
  </entry>
  
  <entry>
    <title>mac工具</title>
    <link href="http://yoursite.com/2017/07/24/mac%E5%B7%A5%E5%85%B7/"/>
    <id>http://yoursite.com/2017/07/24/mac工具/</id>
    <published>2017-07-23T16:16:31.000Z</published>
    <updated>2017-08-14T08:51:34.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>traversable</title>
    <link href="http://yoursite.com/2017/05/23/traversable/"/>
    <id>http://yoursite.com/2017/05/23/traversable/</id>
    <published>2017-05-23T14:56:54.000Z</published>
    <updated>2017-05-23T15:20:16.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="可遍历类"><a href="#可遍历类" class="headerlink" title="可遍历类"></a>可遍历类</h1><p><a href="http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html" target="_blank" rel="external">Coffee</a></p>
<p>Functor     提供给我们一种方法 把普通函数应用到一种高阶类型，并返回高阶类型<br>Applicative is a monoidal functor 提供给我们一种方法 把 高阶类型中的函数 应用到高阶类型，并返回高阶类型。<br>Foldable   提供给我们一种方法 处理一种顺序存储的数据类型 并折叠成一个值</p>
<p>今天我们来理解一下 Traversable type class,首先来看一下 类型类的定义：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">class (Functor t, Foldable t) =&gt; Traversable t where</div><div class="line">    traverse :: Applicative f =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b)</div><div class="line">    traverse f = sequenceA . fmap f</div><div class="line">    sequenceA :: Applicative f =&gt; t (f a) -&gt; f (t a)</div><div class="line">    sequenceA = traverse id</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;可遍历类&quot;&gt;&lt;a href=&quot;#可遍历类&quot; class=&quot;headerlink&quot; title=&quot;可遍历类&quot;&gt;&lt;/a&gt;可遍历类&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;http://adit.io/posts/2013-04-17-functors,_applicative
    
    </summary>
    
    
      <category term="haskell" scheme="http://yoursite.com/tags/haskell/"/>
    
  </entry>
  
  <entry>
    <title>foldRight foldLeft</title>
    <link href="http://yoursite.com/2017/05/21/foldRight-foldLeft/"/>
    <id>http://yoursite.com/2017/05/21/foldRight-foldLeft/</id>
    <published>2017-05-21T12:03:46.000Z</published>
    <updated>2017-05-23T14:53:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>当我们用到FP list我们会接触到list里面大量的工具方法，比如 foreach, map, flatMap, reduce 等等, 但是有一种方法 fold, foldLeft, foldRight 也会大量用到本章主要讲解 fp list中 fold* 的用法。</p>
<p>fold 简单理解就是折叠的意思：</p>
<ul>
<li>foldLeft 是从左边开始折叠</li>
<li>foldRight 是从右边开始折叠</li>
<li>fold 是从右边开始折叠 但是没有初始值</li>
</ul>
<p>那么foldLeft 和 foldRight除了折叠方向不一样呢，还有什么性能差别码？为什么会存在两种不同的折叠方向？ 要了解这些 我们还要先从了解list开始。本文以haskell list 为栗子。其声明为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data [] a = [] | a : [a]</div></pre></td></tr></table></figure></p>
<p>其中(:)为数据构造器，它接受两个参数 head 和 rest of list.<br>当我们在haskell中讨论到数据结构，特别是 list, sequences, trees 我们基本上是谈论 <strong>spine</strong>(脊柱)，可以理解为 脊柱链接集合中的各个值。<br>就拿list来举例， list spine is (:) ,比如 [1,2,3,4] 数据构造过程大概为:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">1 : 2 : 3 : [] </div><div class="line">or</div><div class="line">1 : ( 2 : ( 3 : [] ) ) </div><div class="line">or</div><div class="line">: 1 ( : 2 ( : 3 [] )</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">  : &lt;----------|</div><div class="line"> / \           </div><div class="line">1   : &lt;--------| this is spine</div><div class="line">   / \         </div><div class="line">  2   : &lt;------|</div><div class="line">     / \</div><div class="line">    3  []</div></pre></td></tr></table></figure>
<p>当我们理解list的构造过程的时候，我们通常认为 value 1 首先初始化,然后由(:)去构造， 但是实际上,(:) 包含value 1,<br>正因为如此才能实现 haskell list的惰性求值。 这样spine和value就相互独立，你可以对spine单独遍历，从而必初始化集合元素的值。<br>但我们对构造list集合的时候我们是从右向左运算， 也即是先把3放入[]中，然后往前放入2，往前放入1. 因为haskel list默认是惰性的，所以如果没有用到这个list 他将一直不会求值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b</div><div class="line">foldr f acc []  = acc</div><div class="line">foldr f acc (x:xs) = f x (foldr f acc xs)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">foldl :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; b</div><div class="line">foldl f acc [] = acc</div><div class="line">foldl f acc (x:xs) = foldl f (f acc x) xs</div></pre></td></tr></table></figure>
<p>我们拿map和fold相比较, 你可以基本就认为 fold基本上跟map是一样的，只不过map mapping 函数至每一个元素并返回list，而fold替换(:)至函数并返回一个最终值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">map :: (a-&gt;b) -&gt; [a] -&gt; [b]</div><div class="line">map    (+1)1   :     2 :     3 : []</div><div class="line">       (+1)1   : (+1)2 : (+1)3 : []</div><div class="line">       </div><div class="line">foldr (+) 0 (1 : 2 : 3 : [])</div><div class="line">            (1 + (2 + (3 + 0)))</div><div class="line"></div><div class="line">foldl (+) 0 (1 : 2 : 3 : [])</div><div class="line">          (((0 + 1) + 2) + 3)  </div><div class="line">            </div><div class="line">fold (+) (1 : 2 : 3 : [])</div><div class="line">         (1 + (2 + (3 + 0)))</div></pre></td></tr></table></figure>
<p>在折叠的时候分两个阶段</p>
<ul>
<li>traversal 遍历阶段 是指fold对spine进行递归 </li>
<li>folding   折叠是指 在相应的遍历元素上应用fold函数求值</li>
</ul>
<p>foldr, foldl 对spine的遍历方向都是一样的（从右向左）, 两者的不同是结合顺序(括号的顺序).</p>
<p>foldr的递归定义f 接受list head 和剩余的fold返回值，根据foldr折叠的时候分两个阶段和惰性求值的特性，如果f不要求对第二个参数求值(rest of the fold)这样的话list中的元素就不会再求值<br>这样带来的好处就是 foldr 不仅可以避免对list里的一些或者全部元素求值，还可以避免对list的spine进行遍历。 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">foldr f acc (x:xs) = f x (foldr f acc xs)</div><div class="line">                         ^--------------^</div><div class="line">                         rest of the fold</div></pre></td></tr></table></figure>
<p>list的递归遍历都是有左向右的，不管是foldr or foldl 对list的遍历顺序都是一样的，只是在把函数应用到每个元素并求值的结合顺序不一致，foldr是从右向左应用函数从递归的最里面求值一层一层的最后出来，<br>foldl是从左向右从递归的开始求值直至到最后。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[1..3] == 1 : 2 : 3 : []</div><div class="line"></div><div class="line">foldr f z [1, 2, 3]</div><div class="line">1 `f` (foldr f z [2 ,3])</div><div class="line">1 `f` (2 `f` (foldr z [3]))</div><div class="line">1 `f` (2 `f` (3 `f` (foldr f z [])))</div><div class="line">1 `f` (2 `f` (3 `f` z)</div></pre></td></tr></table></figure></p>
<p>对于 对结合律无关的函数foldr,foldl返回的结果都是一样的，但是对于 结合律有关的函数,比如(^)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">foldr (^) 2 [1..3] </div><div class="line">1 ^ (2 ^ (3 ^ 2))</div><div class="line">1 ^ (2 ^ 9)</div><div class="line">1 ^ 512</div><div class="line">1</div><div class="line"></div><div class="line">foldl (^) 2 [1..3]</div><div class="line">(((2 ^ 1) ^ 2) ^ 3)</div><div class="line">((2 ^ 2) ^ 3)</div><div class="line">(4 ^ 3)</div><div class="line">64</div></pre></td></tr></table></figure></p>
<p>由于以上的特性，所以foldr可以用于那些可能无限长度的list进行操作，但是foldl不适合用于long list。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;当我们用到FP list我们会接触到list里面大量的工具方法，比如 foreach, map, flatMap, reduce 等等, 但是有一种方法 fold, foldLeft, foldRight 也会大量用到本章主要讲解 fp list中 fold* 的用法。&lt;/
    
    </summary>
    
    
      <category term="list" scheme="http://yoursite.com/tags/list/"/>
    
  </entry>
  
  <entry>
    <title>建立个人博客</title>
    <link href="http://yoursite.com/2017/05/21/%E5%BB%BA%E7%AB%8B%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2017/05/21/建立个人博客/</id>
    <published>2017-05-21T09:11:40.000Z</published>
    <updated>2017-07-23T16:11:06.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="github-page-Hexo"><a href="#github-page-Hexo" class="headerlink" title="github-page+Hexo"></a>github-page+Hexo</h1><p><a href="http://www.jianshu.com/p/ecd51e8ef2fa" target="_blank" rel="external">http://www.jianshu.com/p/ecd51e8ef2fa</a><br><a href="https://segmentfault.com/a/1190000004947261" target="_blank" rel="external">https://segmentfault.com/a/1190000004947261</a><br><a href="https://github.com/hexojs/hexo/wiki/Themes" target="_blank" rel="external">https://github.com/hexojs/hexo/wiki/Themes</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;github-page-Hexo&quot;&gt;&lt;a href=&quot;#github-page-Hexo&quot; class=&quot;headerlink&quot; title=&quot;github-page+Hexo&quot;&gt;&lt;/a&gt;github-page+Hexo&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;http:
    
    </summary>
    
    
      <category term="blog" scheme="http://yoursite.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>python2vs3</title>
    <link href="http://yoursite.com/2017/05/21/python2vs3/"/>
    <id>http://yoursite.com/2017/05/21/python2vs3/</id>
    <published>2017-05-21T09:10:05.000Z</published>
    <updated>2017-07-23T16:07:20.000Z</updated>
    
    <content type="html"><![CDATA[<ol>
<li>除法</li>
</ol>
<table>
<thead>
<tr>
<th>python2</th>
<th>python3</th>
</tr>
</thead>
<tbody>
<tr>
<td>10/3 = 3</td>
<td>10/3 = 3.333333 </td>
</tr>
<tr>
<td></td>
<td>10 //3 = 3</td>
</tr>
</tbody>
</table>
<ol>
<li><p>返回闭包时牢记的一点就是：返回函数不要引用任何循环变量，或者后续会发生变化的变量。</p>
</li>
<li><p>此外，我们也需要跟踪程序的执行，查看变量的值是否正确，这个过程称为调试。Python的pdb可以让我们以单步方式执行代码。</p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;除法&lt;/li&gt;
&lt;/ol&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;python2&lt;/th&gt;
&lt;th&gt;python3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10/3 = 3&lt;/td&gt;
&lt;td&gt;10/3 = 3.333
    
    </summary>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>learn database</title>
    <link href="http://yoursite.com/2017/05/21/learn-database/"/>
    <id>http://yoursite.com/2017/05/21/learn-database/</id>
    <published>2017-05-21T09:09:16.000Z</published>
    <updated>2017-07-23T16:07:55.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DataBase"><a href="#DataBase" class="headerlink" title="DataBase"></a>DataBase</h1><p>Relational Database </p>
<h2 id="Advantage"><a href="#Advantage" class="headerlink" title="Advantage"></a>Advantage</h2><ol>
<li>A relational database is an excellent choice for query flexibility.</li>
</ol>
<h2 id="weakness"><a href="#weakness" class="headerlink" title="weakness"></a>weakness</h2><ol>
<li>Partitioning is not one of the strong suits of relational databases like Post- greSQL. </li>
<li>If your data requirements are too flexible to easily fit into the rigid schema requirements of a relational database or you don’t need the overhead of a full database, require very high-volume reads and writes as key values, or need to store only large blobs of data, then one of the other data- stores might be a better fit.</li>
</ol>
<h1 id="Riak-AP"><a href="#Riak-AP" class="headerlink" title="Riak (AP)"></a>Riak (AP)</h1><ol>
<li>Riak is inspired by Amazon’s Dynamo paper</li>
<li>Riak lacks robust support for ad hoc queries, and key-value stores, by design, have trouble linking values together (in other words, they have no foreign keys).</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;DataBase&quot;&gt;&lt;a href=&quot;#DataBase&quot; class=&quot;headerlink&quot; title=&quot;DataBase&quot;&gt;&lt;/a&gt;DataBase&lt;/h1&gt;&lt;p&gt;Relational Database &lt;/p&gt;
&lt;h2 id=&quot;Advantage&quot;&gt;&lt;a
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>learn storm</title>
    <link href="http://yoursite.com/2017/05/21/learn-storm/"/>
    <id>http://yoursite.com/2017/05/21/learn-storm/</id>
    <published>2017-05-21T09:08:00.000Z</published>
    <updated>2017-08-14T08:52:22.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h1><ol>
<li>When designing a topology, one important thing to keep in mind is message reliability. If a message can’t be processed, you need to decide what to do with the individual message and what to do with the topology as a whole. </li>
<li>In Storm, it is the author’s responsibility to guarantee message reliability according to the needs of each topology. </li>
</ol>
<h2 id="Bolt"><a href="#Bolt" class="headerlink" title="Bolt"></a>Bolt</h2><h3 id="Reliable-versus-Unreliable-Bolts"><a href="#Reliable-versus-Unreliable-Bolts" class="headerlink" title="Reliable versus Unreliable Bolts"></a>Reliable versus Unreliable Bolts</h3><ol>
<li>the best way to keep track of the original spout instance is to include a reference to the originating spout in the message tuple. This technique is called <strong>Anchoring</strong>. </li>
<li>A topology is a tree of nodes in which messages (tuples) travel along one or more branches. Each node will ack(tuple) or fail(tuple) so that Storm knows when a mes- sage fails and notifies the spout or spouts that produced the message.</li>
</ol>
<h1 id="Thrift"><a href="#Thrift" class="headerlink" title="Thrift"></a>Thrift</h1><ol>
<li>Its primary goal is to enable efficient and reliable communication across                          programming languages by abstracting the portions of each language that tend to<br>require the most customization into a common library that is implemented in each      language.</li>
<li>Specifically, Thrift allows developers to<br>define datatypes and service interfaces in a single language-neutral<br>file and generate all the necessary code to build RPC clients and<br>servers</li>
<li>A key design choice in the implementation of Thrift was to decouple<br>the transport layer from the code generation layer.</li>
</ol>
<h2 id="Type"><a href="#Type" class="headerlink" title="Type"></a>Type</h2><ol>
<li><p>base types supported by Thrift are:</p>
<ul>
<li>bool A boolean value, true or false</li>
<li>byte A signed byte</li>
<li>i16 A 16-bit signed integer</li>
<li>i32 A 32-bit signed integer</li>
<li>i64 A 64-bit signed integer</li>
<li>double A 64-bit floating point number</li>
<li>string An encoding-agnostic text or binary string</li>
</ul>
</li>
<li><p>Structs <br><br> A struct is essentially equivalent to a class in object oriented<br>programming languages.</p>
</li>
<li><p>Containers <br><br>  containers are strongly typed containers that map to the most<br>commonly used containers in common programming languages.</p>
</li>
<li><p>Exceptions<br><br> Exceptions are syntactically and functionally equivalent to structs<br>except that they are declared using the exception keyword instead<br>of the struct keyword.</p>
</li>
<li><p>Services<br><br> Services are defined using Thrift types. Definition of a service is<br>semantically equivalent to defining an interface (or a pure virtual<br>abstract class) in object oriented programming. </p>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Storm&quot;&gt;&lt;a href=&quot;#Storm&quot; class=&quot;headerlink&quot; title=&quot;Storm&quot;&gt;&lt;/a&gt;Storm&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;When designing a topology, one important thing to ke
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>范畴</title>
    <link href="http://yoursite.com/2017/05/21/%E8%8C%83%E7%95%B4/"/>
    <id>http://yoursite.com/2017/05/21/范畴/</id>
    <published>2017-05-21T08:57:18.000Z</published>
    <updated>2017-07-23T16:10:45.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="范畴"><a href="#范畴" class="headerlink" title="范畴"></a><a href="https://segmentfault.com/a/1190000008000905" target="_blank" rel="external">范畴</a></h1><ol>
<li>用箭头将对象连接起来就可以构造出范畴</li>
<li>范畴论与箭头(函数)的复合有关 </li>
<li>复合是从右向左发生的</li>
<li>复合是可结合的（结合律）</li>
<li>任一对象 A，都有一个箭头，它是复合的最小单位。(id)</li>
<li>一个范畴由对象与箭头（态射）构成。箭头可以复合，这种复合满足结合律。每个对象都有一个恒等箭头，它是箭头复合的基本单位。</li>
<li>在积极阻碍我们探视对象的内部方面，范畴论具有非凡的意义。范畴论中的一个对象，像一个星云。对于它，你所知的只是它与其他对象之间的关系，亦即它与其他对象相连接的箭头。这就是 Internet 搜索引擎对网站进行排名时所用的策略，它只分析输入与输出的链接（除非它受欺骗）.</li>
</ol>
<blockquote>
<p>   在面向对象编程中，一个理想的对象应该是只暴露它的抽象接口（纯表面，无体积），其方法则扮演箭头的角色。如果为了理解一个对象如何与其他对象进行复合，当你发现不得不深入挖掘对象的实现之时，此时你所用的编程范式的原本优势就荡然无存了。</p>
</blockquote>
<h1 id="Catamorphism"><a href="#Catamorphism" class="headerlink" title="Catamorphism"></a>Catamorphism</h1><p>catamorphism: cata 代表 “down” or “against” as in “catacombs”<br>catamorphism 代表结构一中数据结构（表示从一个代数到另一个代数的独一无二的映射）。比如 reduce, fold, sum … 从 list of A to A </p>
<pre><code>polymorphism: 多种形态 
morphism: 形态
</code></pre><h1 id="类型与函数"><a href="#类型与函数" class="headerlink" title="类型与函数"></a>类型与函数</h1><ol>
<li>类型与函数构成的范畴在编程中担任着重要的角色</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;范畴&quot;&gt;&lt;a href=&quot;#范畴&quot; class=&quot;headerlink&quot; title=&quot;范畴&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://segmentfault.com/a/1190000008000905&quot; target=&quot;_blank&quot; rel=&quot;exter
    
    </summary>
    
    
      <category term="category" scheme="http://yoursite.com/tags/category/"/>
    
  </entry>
  
  <entry>
    <title>函数式编程</title>
    <link href="http://yoursite.com/2017/05/21/%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B/"/>
    <id>http://yoursite.com/2017/05/21/函数式编程/</id>
    <published>2017-05-21T08:56:41.000Z</published>
    <updated>2017-08-14T08:54:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>#Closures</p>
<ol>
<li>A closure is a function that carries an implicit binding to all the variables referenced within it. in other words, the function encloses a context around the things it references.</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;#Closures&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A closure is a function that carries an implicit binding to all the variables referenced within it. in other words
    
    </summary>
    
    
      <category term="functional" scheme="http://yoursite.com/tags/functional/"/>
    
  </entry>
  
  <entry>
    <title>learn haskell - advance</title>
    <link href="http://yoursite.com/2017/05/21/learn-haskell-1/"/>
    <id>http://yoursite.com/2017/05/21/learn-haskell-1/</id>
    <published>2017-05-21T08:48:19.000Z</published>
    <updated>2017-07-23T16:06:20.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="15-Monoid-Semigroup"><a href="#15-Monoid-Semigroup" class="headerlink" title="15 Monoid, Semigroup"></a>15 Monoid, Semigroup</h1><h2 id="Algebra"><a href="#Algebra" class="headerlink" title="Algebra"></a>Algebra</h2><ol>
<li>Algebra generally refers to one of the most import fields of mathematics. In this usage, it means the study of mathematical symbols and the rules governing their manipulation.</li>
<li>In Haskell or more generally say in Functional programming, “An algebra” refers to some operations and the set they operate over. that means we care less about the particulars of the values or data we are working with and more about the general rules of the use.</li>
<li>In Haskell, these algebras can be implemented with typeclasses; the typeclasses define the set of operations. when we talk about operations over a set, the set is type the operations are for.</li>
</ol>
<h2 id="Monoid"><a href="#Monoid" class="headerlink" title="Monoid"></a>Monoid</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">class Monoid m where</div><div class="line">	mempty  :: m </div><div class="line">	mappend :: m -&gt; m -&gt; m</div><div class="line">	mconcat :: [m] -&gt; m</div><div class="line">	mconcat = foldr mappend mempty</div></pre></td></tr></table></figure>
<ul>
<li>One of those algebras we use in haskell is Monoid. </li>
<li>the monoidal operation is less about combining the values and more about finding a summary value for the set. Mappending is perhaps best thought of not as a way of combining values in the way that addition or list concatenation does, But as a way to condense any set of values to a summary value.</li>
<li>A monoid is a binary associative operation with an identity.</li>
<li>in plain English, a monoid is a function that takes two arguments and follow two laws:<ol>
<li>associativity</li>
<li>identity</li>
</ol>
</li>
<li>Monoid is the typeclass that generalizes these laws across type.</li>
<li>Monoid is the typeclass just abstracts the pattern(above law) out, giving you the ability to use the operations over a larger range of types.</li>
</ul>
<blockquote>
<p>There is some sense in which it might fell strange to think of this as a combining or <em>mappending</em> operation. Unless we recall that mappending is less about combining and more about condensing or reducing.  i.e. (Bool for monoid)</p>
</blockquote>
<h2 id="common-usage"><a href="#common-usage" class="headerlink" title="common usage"></a>common usage</h2><p>A common use of monoids is to structure and describe common modes of processing data. </p>
<p>Sometimes this is to describe an API for incrementally processing a large dataset, sometimes to describe guarantees needed to roll up aggregations(think summation) in a parallel, concurrent, or distributed processing framework.</p>
<p>One reason is that with a monoid we get another function called mconcat for free. mconcat takes a list of values in a monoid and combines them all together. For example mconcat [a,b,c] is equal to a <code>mappend</code> (b <code>mappend</code> c). Any time you have a monoid you have this quick and easy way to combine a whole list together.(<a href="http://blog.sigfpe.com/2009/01/haskell-monoids-and-their-uses.html" target="_blank" rel="external">http://blog.sigfpe.com/2009/01/haskell-monoids-and-their-uses.html</a>)</p>
<h1 id="Semigroup"><a href="#Semigroup" class="headerlink" title="Semigroup"></a>Semigroup</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">class Semigroup a where</div><div class="line">	(&lt;&gt;) :: a -&gt; a -&gt; a</div></pre></td></tr></table></figure>
<ol>
<li>Compared with Moniod, Semigroup simple drop the identity value, keep associativity and it;s law.</li>
</ol>
<h1 id="Functor"><a href="#Functor" class="headerlink" title="Functor"></a><a href="https://segmentfault.com/a/1190000003954370" target="_blank" rel="external">Functor</a></h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">class Functor f where</div><div class="line">	fmap or &lt;$&gt; :: (a -&gt; b) -&gt; f a -&gt; f b</div><div class="line">	</div><div class="line">Type constants    has kind * </div><div class="line">Type constructors has kind * -&gt; *</div></pre></td></tr></table></figure>
<ol>
<li>A Fnctor is a way to apply a function over or around some structure that we don’t want to later. That is, we want to apply the function to the value that is “inside” some structure and leave the structure alone.</li>
<li>Functor is a typeclass for function application “over” or “through”, or “past” some structure “f” that we want to ignore and leave untouched.</li>
<li>Functor is a way of lifting over structure(mapping) in such a manner that you don’t have to care about the structure because you’re not allowed to touch the structure anyway.</li>
<li>the Goal of the functor fmap is to leave the outer struture untouched while transforming the type arguments inside.</li>
</ol>
<h1 id="函子的复合"><a href="#函子的复合" class="headerlink" title="函子的复合"></a><a href="https://segmentfault.com/a/1190000003954370#articleHeader9" target="_blank" rel="external">函子的复合</a></h1><p>让自己相信范畴之间的函子可以复合并不太难，函子的复合类似于集合之间的函数复合。两个函子的复合，就是两个函子分别对各自的对象进行映射的复合，对于态射也是这样。恒等态射穿过两个函子之后，它还是恒等态射。复合的态射穿过两个函子之后还是复合的态射。函子的复合只涉及这些东西。特别是，自函子很容易复合。还记得 maybeTail 么？下面我用 Haskell 内建的列表来重新实现它（用 [] 替换 Nil，用 : 替换 Cons）：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">maybeTail :: [a] -&gt; Maybe [a]</div><div class="line">maybeTail [] = Nothing</div><div class="line">maybeTail (x:xs) = Just xs</div></pre></td></tr></table></figure>
<p>maybeTail 返回的结果是两个作用于 a 的函子 Maybe 与 [] 复合后的类型。这些函子，每一个都配备了一个 fmap，但是如果我们想将一个函数 f 作用于复合的函子 Maybe [] 所包含的内容，该怎么做？我们不得不突破两层函子的封装：使用 fmap 突破 Maybe，再使用 fmap 突破列表。例如，要对一个 Maybe [Int] 中所包含的元素求平方，可以这样做：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">square x = x * x</div><div class="line"></div><div class="line">mis :: Maybe [Int]</div><div class="line">mis = Just [1, 2, 3]</div><div class="line"></div><div class="line">mis2 = fmap (fmap square) mis</div></pre></td></tr></table></figure>
<p>经过类型分析，对于外部的 fmap，编译器会使用 Maybe 版本的；对于内部的 fmap，编译器会使用列表版本的。于是，上述代码也可以写为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mis2 = (fmap . fmap) square mis</div></pre></td></tr></table></figure>
<h1 id="Monoid-amp-Functor"><a href="#Monoid-amp-Functor" class="headerlink" title="Monoid &amp; Functor"></a>Monoid &amp; Functor</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">-- lifting (a -&gt; b) over f in f a</div><div class="line">fmap  :: Functor     f =&gt;   (a -&gt; b) -&gt; f a        -&gt; f b</div><div class="line"></div><div class="line">apply :: Applicative f =&gt; f (a -&gt; b) -&gt; f a        -&gt; f b</div><div class="line"></div><div class="line">-- binding (a -&gt; m b) over m in m a</div><div class="line">bind  :: Monad       m =&gt; m a        -&gt; (a -&gt; m b) -&gt; m b</div></pre></td></tr></table></figure>
<ol>
<li>Monoid gives us a means of mashing two values of the same type together.<br>Monoid’s core operation, mappend smashes the structures together — when you mappend two list, they become one list, so the structures themselves have been joined.</li>
<li>Functor, on the other hand, if for function application over some structure we don’t want to have to think about.<br>Functor’s core operation, fmap applies a function to a value that is within some structure while leaving that structure unaltered.</li>
</ol>
<h1 id="Applicative"><a href="#Applicative" class="headerlink" title="Applicative"></a>Applicative</h1><ol>
<li>The applicative typeclass allows for function application lifted over structure (like Functor). But with Applicative the function we’re applying is also embedded in some structure.</li>
<li>Applicative is moniodal functor.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">class Functor f =&gt; Applicative f where</div><div class="line">	pure           :: a -&gt; f a</div><div class="line">	(apply or &lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b</div></pre></td></tr></table></figure>
<h1 id="Lifts"><a href="#Lifts" class="headerlink" title="Lifts"></a>Lifts</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">liftA :: Applicative f =&gt; (a  -&gt; b) -&gt; f a  -&gt; f b</div><div class="line">liftM :: Monad       f =&gt; (a1 -&gt; r) -&gt; f a1 -&gt; f r</div><div class="line"></div><div class="line">liftA2 :: Applicative f =&gt; (a-&gt; b-&gt; c) -&gt; f a  -&gt; f b  -&gt; f c</div><div class="line">liftM2 :: Monad       f =&gt; (a1-&gt;a2-&gt;c) -&gt; f a1 -&gt; f a2 -&gt; f c</div></pre></td></tr></table></figure>
<h1 id="Monad"><a href="#Monad" class="headerlink" title="Monad"></a>Monad</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">class Applicative m =&gt; monad m where</div><div class="line">	(&gt;&gt;= or bind)  :: m a -&gt; (a -&gt; m b) -&gt; m b </div><div class="line">	(&gt;&gt;)           :: m a -&gt; m b -&gt; m b</div><div class="line">	return         :: a -&gt; m a</div></pre></td></tr></table></figure>
<ol>
<li>Monads are applicative functors.</li>
<li>chain of dependency <strong>Functor-&gt;Applicative-&gt;Monad</strong>, Whenever you’re implemented an instance of Monoad for a type you necessarily have an Applicative and a Functor as well.</li>
</ol>
<h1 id="Foldable"><a href="#Foldable" class="headerlink" title="Foldable"></a>Foldable</h1><h1 id="Traversable"><a href="#Traversable" class="headerlink" title="Traversable"></a>Traversable</h1><ol>
<li>In a literal sense, Anytime you need to flip two type constructors aroud, or map something and then flip them around, that’s probably Traversable.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">class Traversable t where</div><div class="line">	traverse  :: Applicative f =&gt; (a -&gt; f b) -&gt; t a -&gt; f (t b)</div><div class="line">	sequenceA :: Applicative f =&gt; t (f a) -&gt; f (t a)</div></pre></td></tr></table></figure>
<h1 id="Writer-monad"><a href="#Writer-monad" class="headerlink" title="Writer monad"></a>Writer monad</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">instance (Monoid w) =&gt; Monad (Writer w) where  </div><div class="line">    return x = Writer (x, mempty)  </div><div class="line">    (Writer (x,v)) &gt;&gt;= f = let (Writer (y, v&apos;)) = f x in Writer (y, v `mappend` v&apos;)</div></pre></td></tr></table></figure>
<ol>
<li>Writer Monad is for values that have another value attached that acts as a sort of log value.</li>
</ol>
<h1 id="Reader-monad"><a href="#Reader-monad" class="headerlink" title="Reader monad"></a>Reader monad</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">addStuff :: Int -&gt; Int  </div><div class="line">addStuff x = let  </div><div class="line">    a = (*2) x  </div><div class="line">    b = (+10) x  </div><div class="line">    in a+b</div></pre></td></tr></table></figure>
<ol>
<li>We see that the reader monad allows us to treat functions as values with a context. We can act as if we already know what the functions will return. It does this by gluing functions together into one function and then giving that function’s parameter to all of the functions that it was glued from. So if we have a lot of functions that are all just missing one parameter and they’d eventually be applied to the same thing, we can use the reader monad to sort of extract their future results and the &gt;&gt;= implementation will make sure that it all works out.</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;15-Monoid-Semigroup&quot;&gt;&lt;a href=&quot;#15-Monoid-Semigroup&quot; class=&quot;headerlink&quot; title=&quot;15 Monoid, Semigroup&quot;&gt;&lt;/a&gt;15 Monoid, Semigroup&lt;/h1&gt;&lt;h2
    
    </summary>
    
    
      <category term="haskell" scheme="http://yoursite.com/tags/haskell/"/>
    
  </entry>
  
  <entry>
    <title>learn haskell - basic</title>
    <link href="http://yoursite.com/2017/05/21/learn-haskell-0/"/>
    <id>http://yoursite.com/2017/05/21/learn-haskell-0/</id>
    <published>2017-05-21T08:48:07.000Z</published>
    <updated>2017-07-23T16:05:13.000Z</updated>
    
    <content type="html"><![CDATA[<ol>
<li>Typealias :: </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">type String = [Char]</div></pre></td></tr></table></figure>
<ol>
<li>typeclass :: </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">class TooMany a where</div><div class="line">		tooMany :: a -&gt; Bool</div><div class="line"></div><div class="line">instance TooMany Int where</div><div class="line">		tooMany n = n &gt; 40</div></pre></td></tr></table></figure>
<ol>
<li>datatype  :: </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data MyType = MyData | MyData1 deriving (Eq, Show)</div></pre></td></tr></table></figure>
<ol>
<li>newtype ::  </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">newtype Many = (Int, String)</div></pre></td></tr></table></figure>
<h2 id="Syntax-Rules"><a href="#Syntax-Rules" class="headerlink" title="Syntax Rules"></a>Syntax Rules</h2><ol>
<li>Any operator that starts with a colon (:) must be an infix type or data constructor. all infix data constructors must start with a colon. The type constructor of functions, (-&gt;) is the only infix type constructor that doesn’t start with a colon. Another exception is that they cannot be :: as this syntax is reserved for type assertions.</li>
</ol>
<h2 id="Record-syntax"><a href="#Record-syntax" class="headerlink" title="Record syntax"></a>Record syntax</h2><ol>
<li>Records in Haskell are product types with additional syntax to provide convenient accessors to fields within the recod.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">data Person = Person String Int deriving (Eq, Show) </div><div class="line">data Person = </div><div class="line">   		Person &#123; name :: String,</div><div class="line">   				   age :: Int &#125;</div><div class="line">   				 deriving (Eq, Show)</div></pre></td></tr></table></figure>
<h2 id="ghci"><a href="#ghci" class="headerlink" title="ghci"></a>ghci</h2><ol>
<li>you can using the :{ :} block syntax to write multiple line in REPL.</li>
<li>type :set -Wall when init ghci</li>
<li>use :browse to see a list of the type signatures and functions we loaded from the module xxx .</li>
</ol>
<h1 id="1-basic-expression-and-function"><a href="#1-basic-expression-and-function" class="headerlink" title="1 basic expression and function"></a>1 basic expression and function</h1><ol>
<li>we use :load to  load your test.hs in GHCi</li>
<li>unload the file from ghci, use :m or :module</li>
<li>the order of the declarations in source code doesn’t matter because GHCI loads the entire file at once. but when you enter code one by one in GHCi it’s matters.</li>
<li><em>indentation of the haskell is important</em><br>5.</li>
</ol>
<h2 id="functions"><a href="#functions" class="headerlink" title="functions"></a>functions</h2><ol>
<li>functions are how we factor out the pattern into something we  can reuse with different input. p61</li>
<li>definition function in ghci you have to use <strong>let</strong> , ie: let triple x = x* 3</li>
<li>function start with lowercase, modules and types, ie: Integer start with captial letter</li>
<li><p>you can use functions in an infix or prefix style with a small changes in syntax</p>
<blockquote>
<p>10 <code>div</code> 4</p>
<p>div 10 4</p>
</blockquote>
</li>
<li>associativity and precedence, use :info to check, detail pls visit p67</li>
<li>parenthesizing <strong>infix</strong> functions if you want refer it without apply arugments, or use them as prefix operators. however (-2) is special, try use (2-) instead.</li>
</ol>
<h3 id="where-and-let-in"><a href="#where-and-let-in" class="headerlink" title="where and let..in"></a>where and let..in</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">printInc n = print pluseTwo</div><div class="line">		where plusTwo= n +2</div><div class="line"></div><div class="line">printInc2 n = let plusTwo = n +2</div><div class="line">  		in print plusTwo</div></pre></td></tr></table></figure>
<h4 id="desugaring-let-to-lambda"><a href="#desugaring-let-to-lambda" class="headerlink" title="desugaring let to lambda"></a>desugaring let to lambda</h4><p>printInc2 n = (\plusTwo -&gt; print plusTwo) ( n + 2 )</p>
<h4 id=""><a href="#" class="headerlink" title="($)"></a>($)</h4><p>this is mean evaluate everything to the right of me first, ie: (2^) $ 2+2 = 16</p>
<h1 id="2-strings"><a href="#2-strings" class="headerlink" title="2 strings"></a>2 strings</h1><ol>
<li>type are a way of categorizing values.</li>
<li><em>:type</em> to find out the type of a value, expression or function, the <strong>::</strong>symbol is read as “has the type”, and everything after <strong>::</strong> is our types.</li>
</ol>
<h1 id="3-basic-type"><a href="#3-basic-type" class="headerlink" title="3 basic type"></a>3 basic type</h1><ol>
<li>term level is where your values live and is the code that executes when your program is running . </li>
<li>type level is used during the static analysis &amp; verification of your program.</li>
<li>A typeclass is a set of operations defined with respect to a polymorphic type. when a type is an instance of a typeclass, values of that type can be used in the standard operations defined for that typeclass.</li>
<li>A guideline for differentiating the two kinds of constructors is that type constructors always go to the left of the <strong>=</strong> in a data declaration.</li>
</ol>
<h1 id="Type-Constructor-amp-Data-Constructor"><a href="#Type-Constructor-amp-Data-Constructor" class="headerlink" title="Type Constructor &amp; Data Constructor"></a>Type Constructor &amp; Data Constructor</h1><ol>
<li>Type constructor is used to refer to types which must have arguments applied to become a type.</li>
<li>type constructor is the name of the type and is capitalized When you are reading or writing type signatures(the type level of your code) the type names or type constructors are what you use.</li>
</ol>
<hr>
<ol>
<li>Data constructors in haskell provide a means of creating values that inhabit a given type. </li>
<li>Data constructor are the values that inhabit the type they are defined in. they are values that show up in your code, at the term level instead of the tpe level.</li>
</ol>
<h1 id="4-Advanced-Type"><a href="#4-Advanced-Type" class="headerlink" title="4 Advanced Type"></a>4 Advanced Type</h1><ol>
<li>Every value has a type, Types are how we group a set of values together that share something in common.</li>
<li>(-&gt;) is the type constructor for functions, the value of type(-&gt;) that shows up at term-level is the function. p149</li>
<li>In Haskell, type signatures may have three kinds of types:<ul>
<li>concrete</li>
<li>constrained polymorphic</li>
<li>parametrically polymorphic. ie: function -&gt;  length, id</li>
</ul>
</li>
<li>Haskell;s type inference is built on an extended version of Damas-Hindley-Minler type system.</li>
</ol>
<h2 id="4-1-Newtype"><a href="#4-1-Newtype" class="headerlink" title="4.1 Newtype"></a>4.1 Newtype</h2><ol>
<li>newtype is different in that it permits only one constructor and only one field.</li>
<li>it has no runtime overhead, as it reusees the representation of the type it contains. The differents between newtype and type is contains is gone by the time the compiler generates the code.</li>
<li>{-# GeneralizedNewTypeDeriving #-}  we can use it to reuse the typeclass instance which it contains without define it by our own. </li>
</ol>
<h3 id="why-you-might-use-newtype"><a href="#why-you-might-use-newtype" class="headerlink" title="why you might use newtype"></a>why you might use newtype</h3><ol>
<li>Signal intent: using newtype make it clear that you only intend for it to be a wrapper for the underlying type.</li>
<li>Improve type safety: avoid mixing up many values of the same representation, such as Text or Integer</li>
<li>Add different typeclass instances to a type that is otherwise unchanged representationally.</li>
</ol>
<h3 id="4-2-Type-alias-Synonyms"><a href="#4-2-Type-alias-Synonyms" class="headerlink" title="4.2 Type alias(Synonyms)"></a>4.2 Type alias(Synonyms)</h3><ol>
<li>Try to avoid using type synonyms with unstructured data like text or binary. Type synonyms are best used when you want something lighter weight than <strong>newtypes</strong> but also want your type signatures to be more explicit.</li>
</ol>
<h1 id="5-typeclass"><a href="#5-typeclass" class="headerlink" title="5 typeclass"></a>5 typeclass</h1><ol>
<li>A typeclass is a means of expressing faculties or interfaces that multiple datatypes may have in common. this enables us to write code exclusively in terms of those commonalities without repeating yourself for each instance.</li>
<li>Typeclasses and types in Haskell are, in a sense, opposites. Where a declaration of a type defines how that type in particular is created, a declaration of a typeclass defines how a set of types are consumed or used in computations.</li>
<li>typeclass are being like interfaces in other programming languages.</li>
<li>typeclass allow use to generalize over a set of types in order to define and execute a standard set of features for those types.</li>
<li>types can be made more specific, but not more general or polymorphic. p200</li>
<li>keep your typeclass instances for a type in the same file as that type.</li>
</ol>
<h2 id="side-effect"><a href="#side-effect" class="headerlink" title="side effect"></a>side effect</h2><p>The function is not just applied to the arguments that are in it’s scope but also asked to affect the world outside it’s scope in some way, namely by showing you it’s result on the screen. this is known as a side effect.</p>
<h1 id="6-More-Functional-patterns"><a href="#6-More-Functional-patterns" class="headerlink" title="6 More Functional patterns"></a>6 More Functional patterns</h1><ol>
<li>A value that can be used as an argument to a function is a first-class value.</li>
</ol>
<h2 id="High-Order-Function"><a href="#High-Order-Function" class="headerlink" title="High Order Function"></a>High Order Function</h2><ol>
<li>We were able to rely on the behavior of compare but make changes in the part we wanted to change. This is the value of HOFs. they give us the beginning of a powerful method for reusing and composing code.</li>
</ol>
<h2 id="Point-free-style"><a href="#Point-free-style" class="headerlink" title="Point free style"></a>Point free style</h2><ol>
<li>it’s important to remember that the functions in composition are applied from right to left, like Pacman munching from the right side, reducing the expressions as he goes.</li>
</ol>
<h1 id="11-Alegbraic-datatypes"><a href="#11-Alegbraic-datatypes" class="headerlink" title="11 Alegbraic datatypes"></a>11 Alegbraic datatypes</h1><ol>
<li>data constructors can take arguments. those are guments will be specific types, but not specific values. you can’t say “bool without the possibility of False as a value.”</li>
<li>Alegbraic datatypes in Haskell are algebraic because we can describe the patterns of argument structures using two basic operations: Sum and Product. Chapter 11.7</li>
</ol>
<h2 id="11-1-Kind"><a href="#11-1-Kind" class="headerlink" title="11.1 Kind"></a>11.1 Kind</h2><ol>
<li>Kind are types of type constructors, primarily encoding the number of arguments they take.</li>
<li>kinds are types one level up. they are used to describe the types of type constructors.</li>
<li>the kind * represents a concrete type. There is nothing left awaiting application.</li>
</ol>
<h1 id="12-Maybe-Either-and-Kind"><a href="#12-Maybe-Either-and-Kind" class="headerlink" title="12 Maybe Either and Kind"></a>12 Maybe Either and Kind</h1><h2 id="lifted-and-unlifted-types"><a href="#lifted-and-unlifted-types" class="headerlink" title="lifted and unlifted types"></a>lifted and unlifted types</h2><ol>
<li>To be precise , Kind * is the kind of all standard lifted types, while types that have the kind # are unlifted. A lifted type, which includes any datatype you could define yourself, is any that can be inhabited by bottom.</li>
<li>Unlifted types are any type which cannot be inhabited by bottom. types of kind # are often native machine types and raw pointers. p493</li>
<li></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;Typealias :: &lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td
    
    </summary>
    
    
      <category term="haskell" scheme="http://yoursite.com/tags/haskell/"/>
    
  </entry>
  
</feed>
